#!/usr/bin/env bash
set -e
set -o pipefail

# Print a usage message and exit.
usage() {
	cat >&2 <<'EOF'
To run, I need:
- to be in a container generated by the Dockerfile at the top of the Docker repository;
- to be provided with the name of an S3 bucket, in environment variable AWS_S3_BUCKET;
- to be provided with AWS credentials for this S3 bucket, in environment
	variables AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY;
- a generous amount of good will and nice manners.
The canonical way to run me is to run the image produced by the Dockerfile: e.g.:"
docker run \
    -e AWS_S3_BUCKET=docker.party \
     -e AWS_ACCESS_KEY_ID=... \
     -e AWS_SECRET_ACCESS_KEY=... \
     -it \
     soulshake/blog ./release.sh
EOF
	exit 1
}


echo "Building site with hugo"

if [[ ! -d /usr/src/blog/public ]]; then
		echo "No public folder found."
fi

hugo

[ "$AWS_S3_BUCKET" ] || usage
[ "$AWS_ACCESS_KEY_ID" ] || usage
[ "$AWS_SECRET_ACCESS_KEY" ] || usage

# enter public
cd /usr/src/blog/public

# upload the files to s3
echo "uploading..."
#  Synchronize a directory tree to S3
#      s3cmd sync LOCAL_DIR s3://BUCKET[/PREFIX] or s3://BUCKET[/PREFIX] LOCAL_DIR

s3cmd --verbose \
    --access_key=$AWS_ACCESS_KEY_ID \
    --secret_key=$AWS_SECRET_ACCESS_KEY \
    sync --delete-removed \
    -P /usr/src/blog/public/ s3://$AWS_S3_BUCKET/ 
echo "done."
